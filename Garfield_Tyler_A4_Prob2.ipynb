{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a734f540-2b6f-457e-afcf-33a186c957df",
   "metadata": {},
   "source": [
    "# First adding the boiler plate code to get the seeds and GPU and imports all setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a14c534a-7a8c-4f3a-a077-858c490e3354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "random seed: 1234\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# enable tqdm in pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# set to True to use the gpu (if there is one available)\n",
    "use_gpu = True\n",
    "\n",
    "# select device\n",
    "device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device.type}')\n",
    "\n",
    "# random seed\n",
    "seed = 1234\n",
    "\n",
    "# set random seed\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3a6fc5-3e3a-4219-8e5d-cabc2942b8b5",
   "metadata": {},
   "source": [
    "# Load the average contextualized embeddings generated in problem 1. And the glove words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52328802-ea18-42c4-b8ea-5857586667d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make sure we can load everything properly.\n",
    "embed_tok_id_df = pd.read_csv('datafiles/tok_id_embeds.csv')\n",
    "embed_vecs = np.load('datafiles/cont_embed_vecs.npy')\n",
    "\n",
    "# Now to get back the original data frame.\n",
    "avg_cont_embed_df = pd.DataFrame({'tok_id':embed_tok_id_df['tok_id'].to_numpy(),'avg_cont_embed':list(embed_vecs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a79b59c3-4437-4608-b685-166b490180c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49504, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_cont_embed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3179add6-45b9-42ba-af99-e7ea09214d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tok_id</th>\n",
       "      <th>avg_cont_embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.05390735231835314, 0.08659995588741708, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>133</td>\n",
       "      <td>[-0.06945269355797774, 0.005067573929508234, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>735</td>\n",
       "      <td>[0.0278572527410288, 0.17078800192580665, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34546</td>\n",
       "      <td>[0.0019569261815032732, 0.042429491532477666, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>[0.0823113064755886, 0.21211939386916828, 0.04...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tok_id                                     avg_cont_embed\n",
       "0       0  [-0.05390735231835314, 0.08659995588741708, -0...\n",
       "1     133  [-0.06945269355797774, 0.005067573929508234, 0...\n",
       "2     735  [0.0278572527410288, 0.17078800192580665, -0.0...\n",
       "3   34546  [0.0019569261815032732, 0.042429491532477666, ...\n",
       "4      16  [0.0823113064755886, 0.21211939386916828, 0.04..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(avg_cont_embed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca24055d-0739-4c02-acfd-2eaa1e153421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>chanty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>kronik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>rolonda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>zsombor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>sandberger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word\n",
       "0              the\n",
       "1                ,\n",
       "2                .\n",
       "3               of\n",
       "4               to\n",
       "...            ...\n",
       "399995      chanty\n",
       "399996      kronik\n",
       "399997     rolonda\n",
       "399998     zsombor\n",
       "399999  sandberger\n",
       "\n",
       "[400000 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_words_dict = {'word':[]}\n",
    "\n",
    "with open('datafiles/glove.6B.300d-vocabulary.txt','r') as file:\n",
    "    for line in file:\n",
    "        glove_words_dict['word'].append(line.strip())\n",
    "\n",
    "glove_words_df = pd.DataFrame(glove_words_dict)\n",
    "\n",
    "glove_words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d900e50-45cb-4989-a84e-26cc89b1bd8e",
   "metadata": {},
   "source": [
    "# Perform tokenization on the glove words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8de4cd93-a1b5-4a74-8626-97268f04a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "transformer_name = 'FacebookAI/roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_name, use_fast=True)\n",
    "ignore_index = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec8df01d-889b-4b3c-965d-c45a21cde6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentance(batch):\n",
    "    # First calll tokenizer.\n",
    "    tokenized_sentances = tokenizer(\n",
    "        batch['word'],\n",
    "        truncation=True,\n",
    "        #padding=True\n",
    "        #return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Now add word id's to each of the sentances.\n",
    "    word_ids_lst = []\n",
    "    for i in range(len(batch['word'])):\n",
    "        curr_word_ids = tokenized_sentances.word_ids(batch_index=i)\n",
    "\n",
    "        curr_word_ids = [w_id if w_id!=None else ignore_index for w_id in curr_word_ids]\n",
    "        \n",
    "        word_ids_lst.append(curr_word_ids)\n",
    "    tokenized_sentances['word_ids'] = word_ids_lst\n",
    "\n",
    "    return tokenized_sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32cddedf-5333-4ce1-b8e4-47ae93e30c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b449d046b4ef4d92b4fc6fc1084a083d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "glove_words_ds = Dataset.from_pandas(glove_words_df)\n",
    "\n",
    "glove_words_ds = glove_words_ds.map(tokenize_sentance,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "903bb6ec-b2b0-45f5-8e3f-b79fa66cb1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>word_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>[0, 627, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[-100, 0, -100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>[0, 6, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[-100, 0, -100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.</td>\n",
       "      <td>[0, 4, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[-100, 0, -100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>[0, 1116, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[-100, 0, -100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>[0, 560, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[-100, 0, -100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>chanty</td>\n",
       "      <td>[0, 40805, 219, 2]</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "      <td>[-100, 0, 0, -100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>kronik</td>\n",
       "      <td>[0, 330, 2839, 967, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 0, 0, 0, -100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>rolonda</td>\n",
       "      <td>[0, 9396, 11192, 2]</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "      <td>[-100, 0, 0, -100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>zsombor</td>\n",
       "      <td>[0, 329, 29, 5223, 368, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 0, 0, 0, 0, -100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>sandberger</td>\n",
       "      <td>[0, 39009, 11178, 2]</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "      <td>[-100, 0, 0, -100]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word                   input_ids      attention_mask  \\\n",
       "0              the                 [0, 627, 2]           [1, 1, 1]   \n",
       "1                ,                   [0, 6, 2]           [1, 1, 1]   \n",
       "2                .                   [0, 4, 2]           [1, 1, 1]   \n",
       "3               of                [0, 1116, 2]           [1, 1, 1]   \n",
       "4               to                 [0, 560, 2]           [1, 1, 1]   \n",
       "...            ...                         ...                 ...   \n",
       "399995      chanty          [0, 40805, 219, 2]        [1, 1, 1, 1]   \n",
       "399996      kronik      [0, 330, 2839, 967, 2]     [1, 1, 1, 1, 1]   \n",
       "399997     rolonda         [0, 9396, 11192, 2]        [1, 1, 1, 1]   \n",
       "399998     zsombor  [0, 329, 29, 5223, 368, 2]  [1, 1, 1, 1, 1, 1]   \n",
       "399999  sandberger        [0, 39009, 11178, 2]        [1, 1, 1, 1]   \n",
       "\n",
       "                        word_ids  \n",
       "0                [-100, 0, -100]  \n",
       "1                [-100, 0, -100]  \n",
       "2                [-100, 0, -100]  \n",
       "3                [-100, 0, -100]  \n",
       "4                [-100, 0, -100]  \n",
       "...                          ...  \n",
       "399995        [-100, 0, 0, -100]  \n",
       "399996     [-100, 0, 0, 0, -100]  \n",
       "399997        [-100, 0, 0, -100]  \n",
       "399998  [-100, 0, 0, 0, 0, -100]  \n",
       "399999        [-100, 0, 0, -100]  \n",
       "\n",
       "[400000 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_words_ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f28b7eac-151a-4cbd-86ac-d68b298d14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now convert back to a data frame.\n",
    "glove_words_df = glove_words_ds.to_pandas()\n",
    "avg_cont_emb_ids = np.sort(avg_cont_embed_df['tok_id'].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e291f3e2-205f-4194-914a-331ec052698b",
   "metadata": {},
   "source": [
    "# Now that we have the tokenized glove sub-words and corresponding words the next thing to do is actually loop through all those words and for each gather all the information needed to compute its averaged contextualized embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960e8f0a-adf9-4f53-b796-cc0fa1679adc",
   "metadata": {},
   "source": [
    "This is the main loop that I constructed that will actually performe the word processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6af3def-2863-4c90-8e44-531fecd25837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e79908d8db4b029dd47444a1821cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Missing subwords have occured 0 times thus far!:   0%|          | 0/400000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import RobertaModel\n",
    "mod_embeding_sz = RobertaModel.from_pretrained(transformer_name, add_pooling_layer=False).config.hidden_size\n",
    "# Now we will loop through each row representing each word and for each one where the word_id\n",
    "# isn't -100 I need to get the embedding for the corresponding subword tokin.\n",
    "\n",
    "avg_word_embed = {}\n",
    "missing_subword_tot = 0\n",
    "missing_subw_word = {}\n",
    "\n",
    "prog_bar = tqdm(glove_words_df.iterrows(), total=glove_words_df.shape[0],desc=f\"Missing subwords have occured {missing_subword_tot} times thus far!\",leave=True)\n",
    "\n",
    "for ind,row in prog_bar:\n",
    "    # First access the current word id's and input_ids.\n",
    "    subword_ids = row['input_ids']\n",
    "    word_ids = row['word_ids']\n",
    "    word = row['word']\n",
    "\n",
    "    for i in range(len(subword_ids)):\n",
    "        curr_subword = subword_ids[i]\n",
    "        curr_word = word_ids[i]\n",
    "        \n",
    "        \n",
    "        ind_embed = np.searchsorted(avg_cont_emb_ids,curr_subword)\n",
    "        is_in = ind_embed < len(avg_cont_emb_ids) and avg_cont_emb_ids[ind_embed] == curr_subword\n",
    "        \n",
    "        if curr_subword != -100 and is_in:\n",
    "            # First get the calculated average emneding for the subword_id.\n",
    "            avg_cont_embed_curr = avg_cont_embed_df.loc[avg_cont_embed_df['tok_id']==curr_subword,\\\n",
    "                                                        'avg_cont_embed'].iloc[0]\n",
    "\n",
    "            # Check if sub-word is in dict.\n",
    "            if word not in avg_word_embed:\n",
    "                avg_word_embed[word] = {'tot_oc':0,'cont_embed_sum':np.zeros(mod_embeding_sz)}\n",
    "\n",
    "            # Now word is in dict add it can be recorded.\n",
    "            avg_word_embed[word]['tot_oc'] = avg_word_embed[word]['tot_oc'] + 1\n",
    "            avg_word_embed[word]['cont_embed_sum'] = np.add(avg_word_embed[word]['cont_embed_sum'],\\\n",
    "                                                                     avg_cont_embed_curr)\n",
    "        else:\n",
    "            if word not in missing_subw_word:\n",
    "                missing_subw_word[word] = []\n",
    "\n",
    "            missing_subw_word[word].append(curr_subword)\n",
    "            missing_subword_tot += 1\n",
    "            prog_bar.set_description(f\"Missing subwords have occured {missing_subword_tot} times thus far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5656befa-c0ec-4ed4-8b21-e5d54ecd9b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49504\n",
      "49504\n"
     ]
    }
   ],
   "source": [
    "print(len(list(avg_cont_embed_df['tok_id'])))\n",
    "print(len(set(list(avg_cont_embed_df['tok_id']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "740da413-14ef-4ab5-820d-374c74349467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_subword_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b8452d-f80d-40dd-9bb1-314e1b005881",
   "metadata": {},
   "source": [
    "Ok good so the output below confirms that although some words had subwords that did not apear in the example text that we computed our average embedings on in problem one, each word had at minimum one subword associated with it that we had a vec for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e77e2398-3def-4124-9104-275568e41263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I do want to verify reall qucik that every word has at least one subword contextualized embedding.\n",
    "len(avg_word_embed.keys())==len(glove_words_df['word'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a4628d9-159c-4cd0-a807-7d5ce4f0aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_embed_final = {}\n",
    "\n",
    "for key,value in avg_word_embed.items():\n",
    "    avg_word_embed_final[key] = value['cont_embed_sum'] / value['tot_oc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1086972e-72df-4c7c-93c1-10750a313414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avg_cont_embed_lst = []\n",
    "\n",
    "for value in avg_word_embed_final.values():\n",
    "    avg_cont_embed_lst.append(np.array(value))\n",
    "\n",
    "glove_words_df = pd.DataFrame({'word':glove_words_df['word'].to_list()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aeb52699-bed7-4473-a22c-521507393fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(avg_cont_embed_lst[1][3] == avg_word_embed_final[','][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9a72e1b-bce2-4d68-8bfe-7b211124f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can finally save these average glove embeddings.\n",
    "twod_emb_vecs_np = np.stack(avg_cont_embed_lst)\n",
    "np.save('datafiles/glove_avg_word_vecs.npy',twod_emb_vecs_np)\n",
    "\n",
    "glove_words_df.to_csv('datafiles/glove_words_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bc263d-5b8f-4b67-993c-b67846ee7283",
   "metadata": {},
   "source": [
    "# Loading the averaged contextualized glove embeddings for each glove word into a pandas DF. And then defining the most similar function from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c863cb-32da-4b3e-9942-b266467b1437",
   "metadata": {},
   "source": [
    "### NOTE Run this cell after loading the contextualized embeddings generated in problem 1. This cell loads the saved contextualized embedding to word mapppings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80cac6f3-02e5-4046-8a63-41ecb0cb5780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>avg_cont_embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>[-0.049957829023000076, -0.007305835309671829,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>[-0.01428139524482378, 0.09616818351671312, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.</td>\n",
       "      <td>[-0.05385901583476626, 0.08959653819978437, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>[-0.0032876783164984616, 0.05872345707161294, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>[-0.011179784707645279, 0.05175610889715885, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>chanty</td>\n",
       "      <td>[-0.010604510937390288, 0.10740373668637104, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>kronik</td>\n",
       "      <td>[-0.03847935333754696, 0.10718849114094631, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>rolonda</td>\n",
       "      <td>[-0.017326816602421496, 0.08318891926132678, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>zsombor</td>\n",
       "      <td>[-0.005383800975908686, 0.08314268892852025, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>sandberger</td>\n",
       "      <td>[-0.01658791736203971, 0.050665772139582524, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word                                     avg_cont_embed\n",
       "0              the  [-0.049957829023000076, -0.007305835309671829,...\n",
       "1                ,  [-0.01428139524482378, 0.09616818351671312, 0....\n",
       "2                .  [-0.05385901583476626, 0.08959653819978437, -0...\n",
       "3               of  [-0.0032876783164984616, 0.05872345707161294, ...\n",
       "4               to  [-0.011179784707645279, 0.05175610889715885, -...\n",
       "...            ...                                                ...\n",
       "399995      chanty  [-0.010604510937390288, 0.10740373668637104, 0...\n",
       "399996      kronik  [-0.03847935333754696, 0.10718849114094631, 0....\n",
       "399997     rolonda  [-0.017326816602421496, 0.08318891926132678, -...\n",
       "399998     zsombor  [-0.005383800975908686, 0.08314268892852025, 0...\n",
       "399999  sandberger  [-0.01658791736203971, 0.050665772139582524, -...\n",
       "\n",
       "[400000 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now load the saved files to re-construct a df with all the embeddings.\n",
    "\n",
    "avg_glove_cont_embed = pd.DataFrame({'word':pd.read_csv('datafiles/glove_words_df.csv')['word'].to_list(),\\\n",
    "                                     'avg_cont_embed':list(np.load('datafiles/glove_avg_word_vecs.npy'))})\n",
    "\n",
    "avg_glove_cont_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f258e9f3-809b-4e29-beae-2d9f30255dd8",
   "metadata": {},
   "source": [
    "### Making the index to key and key to index mappings that will be used in the most simiilar function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8d77e39-3982-449f-8057-1b200a9e41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_index = {}\n",
    "index_to_key = {}\n",
    "\n",
    "for ind,row in avg_glove_cont_embed.iterrows():\n",
    "    key_to_index[row['word']]=ind\n",
    "    index_to_key[ind] = row['word']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01301327-f42a-445c-a360-041e4f41954b",
   "metadata": {},
   "source": [
    "Checking the key to index and index to key mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55cc5b7a-0ae7-459e-8b01-3ef5a0243b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 0, ',': 1, '.': 2, 'of': 3, 'to': 4, 'and': 5, 'in': 6, 'a': 7, '\"': 8, \"'s\": 9}\n",
      "\n",
      "\n",
      "{0: 'the', 1: ',', 2: '.', 3: 'of', 4: 'to', 5: 'and', 6: 'in', 7: 'a', 8: '\"', 9: \"'s\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = 10\n",
    "\n",
    "print({key: key_to_index[key] for key in list(key_to_index.keys())[0:n]})\n",
    "print(\"\\n\")\n",
    "print({key: index_to_key[key] for key in list(index_to_key.keys())[0:n]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccecf173-3891-4a50-b768-90deba3a94d8",
   "metadata": {},
   "source": [
    "### Now normalizing the contectualized embeddings that are required for the cosine similarity driving the most similar function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d286a81-a3f3-455f-862a-c4bbae6367cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I need to normalize all the embedding vectors.\n",
    "avg_glove_cont_embed_normed = avg_glove_cont_embed.copy()\n",
    "\n",
    "# Now compute the normed version for each vector.\n",
    "avg_glove_cont_embed_normed['avg_cont_embed'] = avg_glove_cont_embed_normed['avg_cont_embed'].apply(lambda x: x/np.linalg.norm(x))\n",
    "\n",
    "for ind,row in avg_glove_cont_embed_normed.iterrows():\n",
    "    if (1-np.linalg.norm(row['avg_cont_embed'])) >= 0.000001:\n",
    "        print(f'Norm of a vecor was {np.linalg.norm(row['avg_cont_embed'])} FAILED!!!!!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce745b-7bdc-4184-a729-d5ea3a6362bc",
   "metadata": {},
   "source": [
    "### Now defining and actually using the most similar function on the inputs shown in the example emebdding notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "601d01cc-0f46-427d-9d90-ae0bddd28d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I can define a function that uses these normed vectors.\n",
    "def most_similar(word, topn=10):\n",
    "    # Get all the contextualized embedding vectors.\n",
    "    vectors = np.stack(avg_glove_cont_embed_normed['avg_cont_embed'].to_list())\n",
    "    \n",
    "\n",
    "    # Now actually compute the similarity scores.\n",
    "    word_id = key_to_index[word]\n",
    "    word_index = index_to_key.keys()\n",
    "\n",
    "    emb = vectors[word_id]\n",
    "\n",
    "    similarities = vectors @ emb\n",
    "    \n",
    "\n",
    "    # Sort the index of the similarities in ascending order and then reverse that to be descending.\n",
    "    sim_ids_asc = similarities.argsort()\n",
    "    sim_ids_desc = sim_ids_asc[::-1]\n",
    "\n",
    "    # Remove the word that we are comparing the other words to from the list.\n",
    "    mask = sim_ids_desc != word_id\n",
    "    sim_ids_desc = sim_ids_desc[mask]\n",
    "\n",
    "    top_n_sim_ids = sim_ids_desc[:topn]\n",
    "\n",
    "    # Now take the top n most similar emebddings and make a list of tuples where the first part is\n",
    "    # the word and the second part the sim score.\n",
    "    top_n_sim_vals = [(index_to_key[w_id],similarities[w_id]) for w_id in top_n_sim_ids]\n",
    "    return(top_n_sim_vals)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df012f2b-1f0e-4991-a70a-6d801296af2a",
   "metadata": {},
   "source": [
    "# The results of the desired most similar calles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1799332-e84b-4e94-be70-5d7d7a6eff83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('candel', 0.9961354396184285),\n",
       " ('cattle', 0.9959096427882971),\n",
       " ('cotton', 0.9957315960503977),\n",
       " ('ipec', 0.9956895038453432),\n",
       " ('crape', 0.9956829312293125),\n",
       " ('cowers', 0.9956722910406367),\n",
       " ('cress', 0.995654797464526),\n",
       " ('camas', 0.9956196125434017),\n",
       " ('canyon', 0.9955722332729319),\n",
       " ('casher', 0.9955600976599122)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"cactus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "587e1e28-5738-4847-b8e5-18f5bd447bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cakes', 0.9970061178837591),\n",
       " ('beer', 0.9954684669775973),\n",
       " ('tree', 0.9950716231886965),\n",
       " ('hack', 0.994930802911462),\n",
       " ('bread', 0.9949149720361999),\n",
       " ('flake', 0.9948764168639996),\n",
       " ('heart', 0.9948009563419673),\n",
       " ('sticks', 0.9946944092845534),\n",
       " ('axe', 0.9946921708381746),\n",
       " ('rider', 0.9945955090281285)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"cake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13f47f59-0a97-4198-92fd-c1a99e7e950d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ryang', 0.9999999999999999),\n",
       " ('riang', 0.9963455656414293),\n",
       " ('ryong', 0.9961570127608913),\n",
       " ('ryun', 0.9961293893374107),\n",
       " ('ryanggang', 0.9961174751260478),\n",
       " ('reang', 0.9959698522222888),\n",
       " ('angre', 0.9959698522222888),\n",
       " ('layang', 0.995882868618484),\n",
       " ('anyang', 0.9958617792201145),\n",
       " ('angra', 0.9957265234711375)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"angry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8cd4182-04ed-4757-a158-1614f64054b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cleanly', 0.996091148507774),\n",
       " ('closely', 0.9959891183410248),\n",
       " ('quietly', 0.9959157368849155),\n",
       " ('solidly', 0.9958704868535058),\n",
       " ('coldly', 0.9956938703785),\n",
       " ('wildly', 0.9956249683785581),\n",
       " ('smartly', 0.9955571573016364),\n",
       " ('safely', 0.9954547638571951),\n",
       " ('shortly', 0.9953964589136268),\n",
       " ('sweetly', 0.9953727042162738)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"quickly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e5d19f5-db55-4aeb-b547-c4f47bf8e91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inbetween', 0.9916833522764783),\n",
       " ('below', 0.9910037777116857),\n",
       " ('before', 0.9900203016950515),\n",
       " ('during', 0.9899937578302302),\n",
       " ('above', 0.9894070438055993),\n",
       " ('within', 0.9892429289330674),\n",
       " ('about', 0.9891306822007894),\n",
       " ('betweenness', 0.9890898100251142),\n",
       " ('outside', 0.9883810354828868),\n",
       " ('using', 0.9883311756754602)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"between\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e8ad7a3-baf8-4eb0-a5c6-fe7dccfe0779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tuesday.the', 0.9952064044948798),\n",
       " ('it.the', 0.9950649089792694),\n",
       " ('wednesday.the', 0.9948170206480321),\n",
       " ('school.the', 0.9947773667514636),\n",
       " ('people.the', 0.9945479665300365),\n",
       " ('reported.the', 0.9944874079235709),\n",
       " ('region.the', 0.9942999119709547),\n",
       " ('here.the', 0.9942526083900078),\n",
       " ('state.the', 0.9941868098008284),\n",
       " ('area.the', 0.994139669140341)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"the\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
